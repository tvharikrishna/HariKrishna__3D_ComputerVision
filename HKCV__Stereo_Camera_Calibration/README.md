<p align="right">© 𝗗𝗼𝗰𝘂𝗺𝗲𝗻𝘁𝗮𝘁𝗶𝗼𝗻 𝗯𝘆 𝘁𝘃𝗵𝗮𝗿𝗶𝗸𝗿𝗶𝘀𝗵𝗻𝗮</p>
<p align="right">5 𝘮𝘪𝘯𝘶𝘵𝘦 𝘳𝘦𝘢𝘥 📚 </p> <br>

<!------ PROJECT TITLE ------>
<p align="center">
    <img src="readme_data/project_title.png" alt="title" width="1500"/>
</p> <br> <br>

<!------ WHAT ------>
<p align="center">
    <img src="readme_data/what.png" alt="what" width="600"/>
</p>

<p align="center"><h1>🎀 Essence of the Project</h1></p>
<p align='justify'>
Implementation of the Proximal Policy Optimization (PPO) algorithm for a ball balancing slab in reinforcement learning. This technique optimizes the control policy to maintain the ball's position on the slab, adapting to variations in the environment. The goal is to develop a robust and adaptable system that can effectively balance the ball under dynamic conditions, enhancing stability and response accuracy.
</p>

<p align="center">
  <a href="ADD_LINK_HERE">
    <img src="https://img.shields.io/badge/My Project Video-Ball Balancing Slab-blue" alt="Video" width="337" height="30"/>
  </a>
</p> <hr> <br> <br> 

<!------ WHY ------>
<p align="center">
    <img src="readme_data/why.png" alt="What the project accomplishes" width="600"/>
</p>

<p align="center"><h1>🎯 Project Vision</h1></p>
<p style="text-align: justify;">
The project vision for the ball balancing slab using Proximal Policy Optimization (PPO) algorithms in reinforcement learning is to develop advanced control systems that excel in dynamic and unpredictable environments. The aim is to leverage the adaptability and efficiency of PPO algorithms to create a model that balances the ball on the slab with high precision and quickly adjusts to changes, such as external disturbances or varying surface inclinations.
</p> <hr> <br> <br> 

<!------ HOW ------>
<p align="center">
    <img src="readme_data/how.png" alt="What the project accomplishes" width="600"/>
</p>

<p align="center"><h1>🪓Project Implementation</h1></p>
<p><h2>💠 Software Design & Tools </h2></p>
<p align='justify'>
The PPO Ball Balancing project utilizes Unity and C# for game scripting and Python with the ML-Agents package for backend computations. This integration enables the application of the Proximal Policy Optimization (PPO) algorithm, ensuring efficient learning and performance optimization in real-time simulations.
</p>

<p>
<img src="https://img.shields.io/badge/Windows-0078D6.svg?&style=flat-square&logo=windows&logoColor=white" alt="Windows" style="height: 30px;"/> &nbsp;
<img src="https://img.shields.io/badge/Python-3776AB.svg?&style=flat-square&logo=python&logoColor=white" alt="Python" style="height: 30px;"/> &nbsp;
<img src="https://img.shields.io/badge/C%23-004482.svg?&style=flat-square&logo=cplusplus&logoColor=white" alt="C#" style="height: 30px;"/> &nbsp;
<img src="https://img.shields.io/badge/Unity-000000.svg?&style=flat-square&logo=unity&logoColor=white" alt="Unity" style="height: 30px;"/>
</p> <br>

<!------ Deployment and Testing ------>
<p align="center"><h2>💠 Deployment and Testing </h2></p>
<p align='justify'>
<h3>▸ Project Summary: </h3>
The project is designed to demonstrate a reinforcement learning (RL) scenario where an AI agent is tasked with balancing a ball on a slab. The agent receives a small positive reward incrementally for each time step that the ball stays on the slab, encouraging the agent to learn strategies for maintaining balance. 
Conversely, a negative reward is given when the ball falls off, which helps the agent to learn from its mistakes and avoid actions leading to such an outcome. This control mechanismS allows the slab to be rotated along two axes, which adds complexity to the task and requires the agent to develop a nuanced understanding of the physics involved. <br><br>

<p align="center">
    <img src="readme_data/project_title.png" alt="Alt text for your image" width="1500"/>
</p><br>

<h3>▸ About Proximal Policy Optimization (PPO): </h3>
Proximal Policy Optimization (PPO) is a policy gradient method for reinforcement learning which alternates between sampling data through interaction with the environment and optimizing a "surrogate" objective function using stochastic gradient ascent. Developed by OpenAI, PPO aims to improve upon the stability and sample efficiency of previous methods like Trust Region Policy Optimization (TRPO) but with simpler implementation and better general performance. <br><br>

<p align="center">
    <img src="readme_data/project_obs1.png" alt="Alt text for your image" width="1500"/>
</p><br>

<p align="center">
    <img src="readme_data/project_obs2.png" alt="Alt text for your image" width="1500"/>
</p><br>

<h3>▸ Key aspects:</h3>
• Clipped Objective: Limits policy updates to prevent excessive changes. <br>
• Multiple Updates: Allows several mini-batch updates per data sample for better efficiency. <br>
• KL Penalty/Clipping: Ensures policy updates stay within a "safe" range to maintain training stability. <br>
• Advantage: PPO is favored for its simplicity, efficiency, and consistent performance across various RL tasks. <br>
</p>

<!------ Result and Analysis ------>
<p align="center"><h2>💠 Results & Analysis </h2></p>

<p align="center">
    <img src="readme_data/project_obs3.png" alt="Project Observation Image" width="1500"/>
</p>

<p align='justify'>
The analysis of the PPO Ball Balancing project demonstrates successful ball stabilization on the slab with the ability to rapidly adapt to environmental changes. Quantitative metrics and real-time visualizations confirm the algorithm's efficiency and responsiveness, showcasing the practical viability of the system in dynamic scenarios.
</p> <hr> <br> <br> 

<!------ End Image ------>
<p align="center">
    <img src="readme_data/hk_quote.png" alt="Alt text for your image" width="1500"/>
</p>

