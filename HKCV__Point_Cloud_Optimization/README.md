<p align="right">Â© ğ——ğ—¼ğ—°ğ˜‚ğ—ºğ—²ğ—»ğ˜ğ—®ğ˜ğ—¶ğ—¼ğ—» ğ—¯ğ˜† ğ˜ğ˜ƒğ—µğ—®ğ—¿ğ—¶ğ—¸ğ—¿ğ—¶ğ˜€ğ—µğ—»ğ—®</p>
<p align="right">5 ğ˜®ğ˜ªğ˜¯ğ˜¶ğ˜µğ˜¦ ğ˜³ğ˜¦ğ˜¢ğ˜¥ ğŸ“š </p> <br>

<!------ PROJECT TITLE ------>
<p align="center">
    <img src="readme_data/project_title.png" alt="title" width="1500"/>
</p> <br> <br>

<!------ WHAT ------>
<p align="center">
    <img src="readme_data/what.png" alt="what" width="600"/>
</p>

<p align="center"><h1>ğŸ€ Essence of the Project</h1></p>
<p align='justify'>
WRITE HERE
</p>

<p align="center">
  <a href="https://www.youtube.com/watch?v=zDZXDbt0lpU">
    <img src="https://img.shields.io/badge/My Project Video-Point Cloud Optimization-blue" alt="Video" width="420" height="40"/>
  </a>
</p> <hr> <br> <br> 

<!------ WHY ------>
<p align="center">
    <img src="readme_data/why.png" alt="What the project accomplishes" width="600"/>
</p>

<p align="center"><h1>ğŸ¯ Project Vision</h1></p>
<p style="text-align: justify;">
WRITE HERE
</p> <hr> <br> <br> 

<!------ HOW ------>
<p align="center">
    <img src="readme_data/how.png" alt="What the project accomplishes" width="600"/>
</p>

<p align="center"><h1>ğŸª“Project Implementation</h1></p>
<p><h2>ğŸ’  Software Design & Tools </h2></p>
<p align='justify'>
WRITE HERE
</p>

<p>
<img src="https://img.shields.io/badge/Windows-0078D6.svg?&style=flat-square&logo=windows&logoColor=white" alt="Windows" style="height: 30px;"/> &nbsp;
<img src="https://img.shields.io/badge/Python-3776AB.svg?&style=flat-square&logo=python&logoColor=white" alt="Python" style="height: 30px;"/> &nbsp;
<img src="https://img.shields.io/badge/Numpy-013243.svg?&style=flat-square&logo=numpy&logoColor=white" alt="Numpy" style="height: 25px;"/> &nbsp; 
<img src="https://img.shields.io/badge/SciPy-654FF0?style=flat-square&logo=SciPy&logoColor=white" alt="SciPy" style="height: 25px;"/> &nbsp; 
<img src="https://img.shields.io/badge/Pandas-150458.svg?&style=flat-square&logo=pandas&logoColor=white" alt="Pandas" style="height: 25px;"/> &nbsp;
<img src="https://img.shields.io/badge/OpenCV-5C3EE8.svg?&style=flat-square&logo=opencv&logoColor=white" alt="OpenCV" style="height: 25px;"/> &nbsp;
<img src="https://img.shields.io/badge/Open3D-4B8BBE.svg?&style=flat-square&logo=python&logoColor=white" alt="Open3D" style="height: 25px;"/> &nbsp;
<img src="https://img.shields.io/badge/Mesh-000000.svg?&style=flat-square&logo=blender&logoColor=white" alt="Mesh" style="height: 25px;"/>
</p> <br>

<!------ Deployment and Testing ------>
<p align="center"><h2>ğŸ’  Deployment and Testing </h2></p>
<p align='justify'>
<h3>â–¸ Project Summary: </h3>
The project is designed to demonstrate a reinforcement learning (RL) scenario where an AI agent is tasked with balancing a ball on a slab. The agent receives a small positive reward incrementally for each time step that the ball stays on the slab, encouraging the agent to learn strategies for maintaining balance. 
Conversely, a negative reward is given when the ball falls off, which helps the agent to learn from its mistakes and avoid actions leading to such an outcome. This control mechanismS allows the slab to be rotated along two axes, which adds complexity to the task and requires the agent to develop a nuanced understanding of the physics involved. <br><br>

<p align="center">
    <img src="readme_data/project_title.png" alt="Alt text for your image" width="1500"/>
</p><br>

<h3>â–¸ About Proximal Policy Optimization (PPO): </h3>
Proximal Policy Optimization (PPO) is a policy gradient method for reinforcement learning which alternates between sampling data through interaction with the environment and optimizing a "surrogate" objective function using stochastic gradient ascent. Developed by OpenAI, PPO aims to improve upon the stability and sample efficiency of previous methods like Trust Region Policy Optimization (TRPO) but with simpler implementation and better general performance. <br><br>

<p align="center">
    <img src="readme_data/project_obs1.png" alt="Alt text for your image" width="1500"/>
</p><br>

<p align="center">
    <img src="readme_data/project_obs2.png" alt="Alt text for your image" width="1500"/>
</p><br>

<h3>â–¸ Key aspects:</h3>
â€¢ Clipped Objective: Limits policy updates to prevent excessive changes. <br>
â€¢ Multiple Updates: Allows several mini-batch updates per data sample for better efficiency. <br>
â€¢ KL Penalty/Clipping: Ensures policy updates stay within a "safe" range to maintain training stability. <br>
â€¢ Advantage: PPO is favored for its simplicity, efficiency, and consistent performance across various RL tasks. <br>
</p>

<!------ Result and Analysis ------>
<p align="center"><h2>ğŸ’  Results & Analysis </h2></p>

<p align="center">
    <img src="readme_data/project_obs3.png" alt="Project Observation Image" width="1500"/>
</p>

<p align='justify'>
The analysis of the PPO Ball Balancing project demonstrates successful ball stabilization on the slab with the ability to rapidly adapt to environmental changes. Quantitative metrics and real-time visualizations confirm the algorithm's efficiency and responsiveness, showcasing the practical viability of the system in dynamic scenarios.
</p> <hr> <br> <br> 

<!------ End Image ------>
<p align="center">
    <img src="readme_data/hk_quote.png" alt="Alt text for your image" width="1500"/>
</p>

